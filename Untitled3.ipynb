{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWmGi/DsSimzzwCYppBAVm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abha989/Python-Assignments/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtDi0Yz5gqGC"
      },
      "outputs": [],
      "source": [
        "Q1- What is a Decision Tree, and how does it work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' A decision tree is a flowchart-like diagram used to map out potential solutions to a problem and make decisions based on a set of rules.\n",
        "It works by breaking down a complex problem into smaller, more manageable sub-problems, each represented by a node in the tree. The tree starts with a root node\n",
        " and branches out based on different conditions or features, ultimately leading to leaf nodes that represent the final decisions or predictions."
      ],
      "metadata": {
        "id": "57LyFBagg1qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q2- What are impurity measures in Decision Trees"
      ],
      "metadata": {
        "id": "QMxZWT7Og4jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Impurity measures in decision trees quantify the homogeneity of classes within a node. They are used to determine the best split for a node by evaluating the\n",
        "reduction in impurity after splitting. Two common impurity measures are Gini impurity and entropy.\n"
      ],
      "metadata": {
        "id": "a0NnujHtg9b5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3- What is the mathematical formula for Gini Impurity?"
      ],
      "metadata": {
        "id": "ZBmYn7PehEC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The Gini Impurity formula is used in decision tree algorithms to measure the impurity or randomness within a dataset or node. It is defined as:\n",
        "Gini(D) = 1 - ∑_{i=1}^{C} p_i^2\n",
        "Where:\n",
        "D is the set of data points.\n",
        "C is the number of classes in the dataset.\n",
        "p_i is the proportion (or probability) of data points belonging to class i.\n",
        "Under the Hood: Gini Impurity. This article will serve as ...\n",
        "Essentially, Gini Impurity calculates the probability that two randomly chosen data points from the set will be misclassified.\n",
        "A lower Gini Impurity indicates a more homogeneous set, meaning the data points are more likely to belong to the same class."
      ],
      "metadata": {
        "id": "In-T9siBhM3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4- What is the mathematical formula for Entropy?"
      ],
      "metadata": {
        "id": "_5kZ7ksthPuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' n one statistical interpretation of entropy, it is found that for a very large system in thermodynamic equilibrium, entropy S is proportional to the natural\n",
        "logarithm of a quantity Ω representing the maximum number of microscopic ways in which the macroscopic state corresponding to S can be realized; that is, S = k\n",
        "ln .."
      ],
      "metadata": {
        "id": "InaHnFpQhVkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5- What is Information Gain, and how is it used in Decision Trees"
      ],
      "metadata": {
        "id": "OLcy90dFheDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Information Gain (IG) is a metric used in decision tree algorithms to determine the best attribute for splitting a dataset. It measures the reduction in entropy (uncertainty) of a target variable when a specific feature is known, according to GeeksforGeeks. In essence, it quantifies how much a feature helps in classifying the data.\n",
        "Here's how Information Gain is used in decision trees:\n",
        "1. Calculating Entropy:\n",
        "Entropy measures the impurity or randomness of a dataset. A dataset with high entropy has a more mixed distribution of classes, while a dataset with low entropy is more homogeneous.\n",
        "2. Calculating Information Gain:\n",
        "Information Gain is calculated by subtracting the weighted average entropy of the child nodes (after a split) from the entropy of the parent node (before the split). A larger Information Gain indicates a more effective split, meaning the chosen feature helps reduce uncertainty in the target variable more significantly.\n",
        "3. Choosing the Best Attribute:\n",
        "During the decision tree building process, the algorithm calculates the Information Gain for each feature at each node. The feature with the highest Information Gain is selected as the splitting attribute for that node, explains Wikipedia.\n",
        "4. Building the Tree:\n",
        "This process of calculating and selecting the best splitting attribute is repeated recursively for each node until a stopping criterion is met (e.g., all data points in a node belong to the same class or a maximum tree depth is reached)"
      ],
      "metadata": {
        "id": "x96LSMDEhkib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6- What is the difference between Gini Impurity and Entropy"
      ],
      "metadata": {
        "id": "dEMS4-yihrhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Gini impurity and entropy are both measures of impurity used in decision tree algorithms to determine the best split for a node. Gini impurity is a measure of how often a randomly chosen element from a set would be incorrectly classified, while entropy is a measure of the randomness or disorder in a set.\n",
        "Here's a more detailed comparison:\n",
        "1. Meaning:\n",
        "Gini Impurity:\n",
        "Represents the probability of misclassifying a randomly selected element if it were classified based on the current class distribution in a node. A lower Gini impurity indicates a more homogeneous node, meaning the classes are more clearly separated.\n",
        "Entropy:\n",
        "Measures the randomness or uncertainty in a set of data. A higher entropy indicates more randomness, meaning the classes are more evenly distributed.\n",
        "2. Formula:\n",
        "Gini Impurity:\n",
        "Gini = 1 - Σ (pᵢ)² , where pᵢ is the probability of an element belonging to class i.\n",
        "Entropy:\n",
        "Entropy = - Σ (pᵢ * log₂ pᵢ), where pᵢ is the probability of an element belonging to class i.\n",
        "3. Ranges:\n",
        "Gini Impurity: Ranges from 0 to 0.5.\n",
        "Entropy: Ranges from 0 to log₂(number of classes).\n",
        "4. Computational Cost:\n",
        "Gini Impurity: Computationally cheaper than entropy because it involves simple arithmetic, while entropy uses logarithms.\n",
        "Entropy: More computationally expensive due to the logarithm calculation.\n",
        "5. Performance:\n",
        "Gini Impurity: Generally performs similarly to entropy in most cases.\n",
        "Entropy: May lead to slightly more balanced decision trees in some cases, but the difference is often negligible.\n",
        "6. Interpretation:\n",
        "Gini Impurity:\n",
        "Some find Gini impurity easier to interpret intuitively because it directly relates to the probability of misclassification.\n",
        "Entropy:\n",
        "Entropy is more closely related to the concept of information gain, which measures the reduction in uncertainty after a split.\n",
        "In summary: Gini impurity and entropy are both valid measures of impurity used in decision trees. Gini impurity is generally faster to compute and may be slightly easier to interpret, while entropy may lead to slightly more balanced trees in some cases. The choice between them often comes down to a matter of computational efficiency and individual preference."
      ],
      "metadata": {
        "id": "SaHB4GRkh2qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7- What is the mathematical explanation behind Decision Trees"
      ],
      "metadata": {
        "id": "IRXfcIq1h4yW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Decision trees, at their core, employ mathematical concepts to guide the splitting of data and determine which features are most important for making predictions. Key mathematical ideas include information gain, entropy, and Gini impurity, used to measure the purity and helpfulness of data splits.\n",
        "1. Information Gain:\n",
        "Entropy:\n",
        "Represents the randomness or impurity of a dataset. A high entropy value indicates a dataset with mixed classes, while a low entropy value indicates a dataset where one class dominates.\n",
        "Information Gain:\n",
        "Measures the reduction in entropy after splitting a dataset based on a particular feature. It quantifies how much a feature helps in separating different classes.\n",
        "Formula:\n",
        "IG(T, a) = Entropy(T) - Weighted Sum of Entropy(T | a), where T is a node, a is a feature, and the weighted sum considers the proportions of data in each child node after splitting.\n",
        "2. Gini Impurity:\n",
        "Gini Impurity:\n",
        "Another measure of impurity, similar to entropy, but often computationally faster to calculate.\n",
        "Formula:\n",
        "Gini = 1 - Σ (probability of each class)^2. A lower Gini value indicates a more pure node (e.g., a node with mostly one class).\n",
        "3. Attribute Selection:\n",
        "Greedy Algorithm:\n",
        "Decision trees often use a greedy algorithm, which at each split, selects the feature that maximizes information gain or minimizes Gini impurity.\n",
        "Splitting Criteria:\n",
        "The goal is to find the best splitting criterion (feature and threshold) that results in the most homogeneous child nodes.\n",
        "4. Regression Trees:\n",
        "Variance: In regression trees, variance is often used as the splitting criterion.\n",
        "Splitting: The algorithm aims to find the split that minimizes the variance (average squared difference from the mean) in the target variable.\n",
        "5. Complexity and Balancing:\n",
        "Overfitting:\n",
        "Decision trees can be prone to overfitting, meaning they learn the training data too well and perform poorly on new, unseen data.\n",
        "Pruning:\n",
        "Techniques like pruning can be used to simplify the tree and prevent overfitting.\n",
        "Balanced vs. Unbalanced Trees:\n",
        "The mathematical properties of balanced trees are well-defined (e.g., cost of construction and query time), but decision trees are not always balanced, so subtrees may need to be considered approximately balanced for analysis."
      ],
      "metadata": {
        "id": "eR0Ox_oVh9EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8- What is Pre-Pruning in Decision Trees"
      ],
      "metadata": {
        "id": "CfGaVRw4iGWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Pre-pruning in decision trees, also known as early stopping, is a technique used to prevent overfitting by limiting the growth of the decision tree before\n",
        "it reaches its maximum potential complexity. It works by setting criteria for halting the tree-building process during the construction phase."
      ],
      "metadata": {
        "id": "xC8dxI55iKId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q9- * What is Post-Pruning in Decision Trees"
      ],
      "metadata": {
        "id": "TX5yX9iUiScO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Post-pruning in decision trees involves trimming branches of a fully grown tree to improve its ability to generalize and reduce overfitting.\n",
        "It's done after the tree has been fully constructed, allowing it to overfit the training data. Post-pruning can be effective in simplifying complex trees without\n",
        "significantly reducing accuracy, ultimately leading to a model that performs better on unseen data."
      ],
      "metadata": {
        "id": "_-4i3sl-iV23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q10- What is the difference between Pre-Pruning and Post-Pruning"
      ],
      "metadata": {
        "id": "7ezMFUp5ienj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Pre-pruning and post-pruning are two techniques used to control the complexity of decision trees in machine learning. Pre-pruning, also known as early stopping, limits the growth of the tree during its construction, preventing it from becoming overly complex. Post-pruning, on the other hand, simplifies a fully grown tree by removing branches that don't significantly contribute to model performance.\n",
        "Pre-Pruning (Early Stopping):\n",
        "Timing: Stops the tree's growth before it reaches its full potential.\n",
        "How it works: Uses predetermined criteria (e.g., maximum depth, minimum number of samples per leaf) to halt the expansion of branches.\n",
        "Effect: Can prevent overfitting by limiting the tree's complexity, but might also lead to underfitting if the constraints are too strict.\n",
        "Post-Pruning (Reducing Nodes):\n",
        "Timing: Applies after the tree has been fully constructed.\n",
        "How it works: Removes branches that don't significantly improve prediction accuracy, based on a cost-complexity parameter or other pruning criteria.\n",
        "Effect: Can improve generalization by simplifying the tree while preserving its accuracy, but might require more computational resources for pruning the tree.\n"
      ],
      "metadata": {
        "id": "nbv28oxkiiXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q11-What is a Decision Tree Regressor"
      ],
      "metadata": {
        "id": "a6-Yn-S3incl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Decision tree regression is a supervised machine learning technique used to predict continuous numerical values. It creates a tree-like model that splits\n",
        "data based on features, allowing it to make predictions about numerical outcomes. Unlike classification tasks where the output is categorical, regression trees\n",
        "focus on estimating numerical values."
      ],
      "metadata": {
        "id": "nHoXrdtGiwRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q12- What are the advantages and disadvantages of Decision Trees"
      ],
      "metadata": {
        "id": "A5lf80KuizLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Decision trees, a versatile machine learning algorithm, offer benefits like interpretability, adaptability to various data types, and the ability to handle missing values. However, they also have drawbacks, including potential overfitting, instability, and difficulties with complex, non-linear relationships.\n",
        "Advantages:\n",
        "Interpretability: Decision trees are easy to understand and visualize, making them suitable for scenarios where transparency and human understanding are crucial.\n",
        "Versatility: They can be used for both classification and regression tasks, and they don't require extensive data preparation or scaling.\n",
        "Handles Non-Linear Relationships: Decision trees can effectively model complex, non-linear patterns in the data.\n",
        "Robust to Outliers: They are relatively insensitive to outliers in the dataset.\n",
        "Can Handle Missing Values: Decision trees can be trained and used even with missing data points.\n",
        "Disadvantages:\n",
        "Overfitting:\n",
        "Decision trees can learn the training data too well, leading to poor generalization on new, unseen data.\n",
        "Instability:\n",
        "Small changes in the training data can lead to significant variations in the tree structure and predictions.\n",
        "Bias towards Dominant Classes:\n",
        "In imbalanced datasets, decision trees may favor the majority class, leading to poor performance on minority classes.\n",
        "Difficulty with Complex, Non-Linear Relationships:\n",
        "While they excel at non-linear relationships, decision trees can struggle with complex, smooth, or continuous patterns.\n",
        "Requires Enough Data:\n",
        "Decision trees require a sufficient amount of data to learn effective splits and make accurate predictions.\n",
        "Computational Complexity:\n",
        "Building and using decision trees can be computationally expensive, especially on large datasets.\n",
        "Not Suitable for High-Dimensional Data:\n",
        "Decision trees may not be the best choice for datasets with a large number of features or variables."
      ],
      "metadata": {
        "id": "J4NiByqHi20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q13- How does a Decision Tree handle missing values\t?"
      ],
      "metadata": {
        "id": "-kSqtMLli9km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Decision Trees can handle missing values in several ways, depending on the implementation and settings used. Here's a breakdown of the common strategies:\n",
        "\n",
        "1. Ignoring Instances with Missing Values (Simple, but Lossy)\n",
        "Some basic decision tree implementations will simply discard rows (data points) with missing values during training. This approach is simple but may result in a loss of valuable data.\n",
        "\n",
        "2. Surrogate Splits (Used in CART and C4.5)\n",
        "When a feature value is missing at a decision node, the tree can use a surrogate split: an alternate feature that provides a similar split.\n",
        "\n",
        "For example, if a node splits on \"age\" but \"age\" is missing, it may use \"income\" instead if historically \"income\" often aligns with how \"age\" splits the data.\n",
        "\n",
        "Pros: Keeps data and uses other relevant information.\n",
        "Cons: Computationally more complex.\n",
        "\n",
        "3. Probabilistic Splitting (Used in some advanced implementations)\n",
        "If a feature value is missing, the instance is sent down all branches, weighted by the proportion of training data that went down each branch.\n",
        "\n",
        "For example, if 70% of training data with known values went left and 30% went right, the current sample is split accordingly.\n",
        "\n",
        "Pros: Keeps more data and accounts for uncertainty.\n",
        "Cons: Makes tree interpretation less intuitive.\n",
        "\n",
        "4. Imputation Before Training\n",
        "Often, missing values are filled (imputed) before training using strategies like:\n",
        "\n",
        "Mean/median/mode imputation\n",
        "\n",
        "k-Nearest Neighbors imputation\n",
        "\n",
        "Model-based imputation (e.g. regression)\n",
        "\n",
        "Pros: Clean input data for tree; widely supported.\n",
        "Cons: Risk of introducing bias if not done carefully.\n",
        "\n",
        "5. Built-in Handling in Libraries\n",
        "Different libraries handle this differently:\n",
        "\n",
        "scikit-learn: Prior to version 0.24, did not support missing values directly. Now, HistGradientBoosting supports NaNs.\n",
        "\n",
        "XGBoost: Automatically learns the best direction to take when encountering a missing value.\n",
        "\n",
        "LightGBM: Similar to XGBoost, has built-in handling for missing values."
      ],
      "metadata": {
        "id": "1wgeMNozjC8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q14- How does a Decision Tree handle categorical features"
      ],
      "metadata": {
        "id": "1OGUPeQujTrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Decision trees inherently handle categorical features by splitting data based on the categories within those features. Unlike continuous features where splits are based on thresholds, decision trees directly compare data points to specific categories during the splitting process.\n",
        "Here's a more detailed explanation:\n",
        "1. Splitting based on Categories:\n",
        "Decision trees can split on categorical features without needing to convert them into numerical values beforehand, though preprocessing techniques like one-hot encoding or label encoding can be used for specific scenarios.\n",
        "At each node, the algorithm evaluates which categorical feature provides the best split based on a chosen criterion (e.g., Gini impurity, information gain).\n",
        "The split is determined by comparing data points to specific categories within the selected feature.\n",
        "2. Algorithm's Approach:\n",
        "The decision tree algorithm identifies the best feature to split on, whether it's categorical or continuous.\n",
        "For categorical features, the split involves creating branches based on the presence or absence of specific categories.\n",
        "For continuous features, the split involves finding a threshold value that divides the data into two or more groups.\n",
        "This process continues recursively until a stopping criterion is met (e.g., a maximum tree depth, a minimum number of samples in a leaf node).\n",
        "3. Importance of Feature Selection:\n",
        "The decision tree algorithm selects the feature that leads to the most homogeneous subsets after splitting.\n",
        "This selection process helps ensure that the resulting tree is as accurate and interpretable as possible.\n",
        "4. Preprocessing Techniques:\n",
        "While decision trees can handle categorical features natively, preprocessing techniques can be beneficial in certain situations.\n",
        "One-hot encoding: Converts categorical features into binary columns, creating a new column for each category.\n",
        "Label encoding: Assigns numerical labels to each category, allowing for a direct comparison of values.\n",
        "These techniques can improve model performance in specific cases, especially when dealing with high-cardinality categorical features.\n",
        "5. Challenges and Considerations:\n",
        "High cardinality:\n",
        "Categorical features with many unique values can create numerous splits, potentially leading to overfitting or complexity.\n",
        "Missing values:\n",
        "Missing values in categorical features can be handled using imputation strategies like using the most frequent category or creating a unique \"Missing\" category.\n",
        "Model interpretability:\n",
        "Decision trees are generally easy to interpret, but highly complex trees can be harder to understand.\n",
        "Feature selection:\n",
        "Not all categorical features are equally informative, so feature selection techniques can help identify the most important features for the model.\n",
        "Decision Tree Algorithm\n",
        "In essence, decision trees provide a flexible approach to handling categorical features, allowing for both direct splitting based on categories and the use of preprocessing techniques to address specific challenges."
      ],
      "metadata": {
        "id": "Tj9_-x-9jkw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q15- What are some real-world applications of Decision Trees?"
      ],
      "metadata": {
        "id": "iVTx5uVyjmih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Decision trees find applications in diverse fields like finance, healthcare, and marketing, aiding in tasks such as predicting customer churn, identifying fraudulent transactions, and making medical diagnoses. They are also used for tasks like credit scoring, identifying high-risk customers, and predicting stock market movements.\n",
        "Here's a more detailed look:\n",
        "Finance:\n",
        "Credit Scoring:\n",
        "Decision trees analyze applicant data (income, debt, credit history) to assess loan eligibility and determine risk levels according to a bank's report.\n",
        "Fraud Detection:\n",
        "Identifying suspicious transactions and potentially fraudulent activity according to Kanerika.\n",
        "Option Pricing:\n",
        "Evaluating and pricing options based on various factors like time to expiration and interest rates.\n",
        "Risk Assessment:\n",
        "Assessing the risk associated with different investment strategies or financial products.\n",
        "Stock Market Forecasting:\n",
        "Predicting future stock market movements based on historical data and other relevant factors.\n",
        "Healthcare:\n",
        "Medical Diagnosis:\n",
        "Assisting doctors in diagnosing diseases by creating models that suggest diagnoses based on patient symptoms and test results according to Ultralytics.\n",
        "Patient Outcome Prediction:\n",
        "Predicting patient outcomes based on medical history and treatment plans according to Lyzr.\n",
        "Resource Allocation:\n",
        "Helping hospitals optimize resource allocation based on patient needs and predicted outcomes.\n",
        "Marketing:\n",
        "Customer Churn Prediction:\n",
        "Identifying customers likely to stop using a service and implementing retention strategies according to Ultralytics.\n",
        "Targeted Marketing:\n",
        "Segmenting customers based on behavior and preferences to personalize marketing campaigns.\n",
        "Product Recommendation:\n",
        "Recommending products or services to customers based on their past purchases or browsing history.\n",
        "Identifying Factors Leading to Better Gross Margins:\n",
        "Analyzing data to understand which factors contribute to a retail chain's success.\n",
        "Other Industries:\n",
        "Predicting High Occupancy Dates for Hotels: Forecasting peak demand periods for hotels based on various factors like seasonality and events according to Quora.\n",
        "Crime Risk Assessment: Identifying neighborhoods or areas with a higher risk of crime based on historical data.\n",
        "Failure Prediction: Predicting potential failures in manufacturing or other processes.\n",
        "Engineering and Civil Planning: Using decision trees for various engineering and civil planning tasks."
      ],
      "metadata": {
        "id": "CmC5Xrq8jp79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Practical"
      ],
      "metadata": {
        "id": "09oW1UeTjxup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q16- Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy*"
      ],
      "metadata": {
        "id": "dqh6gn4Nj0Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data      # Features\n",
        "y = iris.target    # Labels\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er5O2PjNj3iE",
        "outputId": "37f6583f-aadc-4fbd-bc40-f9866eb87623"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q17-  Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "feature importances*"
      ],
      "metadata": {
        "id": "2WukdT4hj_0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree using Gini impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances (using Gini impurity):\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "N-V-qGabkC_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q18- Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy"
      ],
      "metadata": {
        "id": "IYEQ9o3zmMUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy using Entropy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "ai01BqlmmPhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q19- Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)*"
      ],
      "metadata": {
        "id": "iWXM_2mDmXCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create the Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE) on Test Set: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "TxqWlsSZmaPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q20- Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz*"
      ],
      "metadata": {
        "id": "s0hsHWo5mjbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "from graphviz import Source\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree to DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize using graphviz\n",
        "graph = Source(dot_data)\n",
        "graph.render(\"iris_tree\", format=\"png\", cleanup=True)  # Saves as iris_tree.png\n",
        "graph.view()\n"
      ],
      "metadata": {
        "id": "Cs0TdFKgmnj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q21- Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree*"
      ],
      "metadata": {
        "id": "lOkYT_2Hmws2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Decision Tree with max depth of 3\n",
        "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_depth3.fit(X_train, y_train)\n",
        "y_pred_depth3 = clf_depth3.predict(X_test)\n",
        "accuracy_depth3 = accuracy_score(y_test, y_pred_depth3)\n",
        "\n",
        "# Model 2: Fully grown Decision Tree (no max depth)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy (Max Depth = 3): {accuracy_depth3 * 100:.2f}%\")\n",
        "print(f\"Accuracy (Fully Grown Tree): {accuracy_full * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "iRdj2W5Um0Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q22- Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree*"
      ],
      "metadata": {
        "id": "yUQbJhsTm9Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model 1: Decision Tree with min_samples_split=5\n",
        "clf_split5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "clf_split5.fit(X_train, y_train)\n",
        "y_pred_split5 = clf_split5.predict(X_test)\n",
        "accuracy_split5 = accuracy_score(y_test, y_pred_split5)\n",
        "\n",
        "# Model 2: Default Decision Tree\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "clf_default.fit(X_train, y_train)\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy (min_samples_split=5): {accuracy_split5 * 100:.2f}%\")\n",
        "print(f\"Accuracy (Default Tree): {accuracy_default * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "XsT9Y2q3nAbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q23-Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data*"
      ],
      "metadata": {
        "id": "hKqSRPXjnG_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Model 1: Train on Unscaled Data -----------\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# ----------- Model 2: Train on Scaled Data -----------\n",
        "# Apply standard scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ----------- Output Accuracy Comparison -----------\n",
        "print(f\"Accuracy (Unscaled Data): {accuracy_unscaled * 100:.2f}%\")\n",
        "print(f\"Accuracy (Scaled Data):   {accuracy_scaled * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "O6J7tGXrnSO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q24- Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification*\n"
      ],
      "metadata": {
        "id": "-0GzlSZjnT7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Wrap it in One-vs-Rest strategy\n",
        "ovr_clf = OneVsRestClassifier(dt)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = ovr_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with One-vs-Rest Decision Tree: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "gfg1za4RnZZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q25- Write a Python program to train a Decision Tree Classifier and display the feature importance scores*"
      ],
      "metadata": {
        "id": "_OihF1k9ndTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importance Scores:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "HjT6x4yGnhCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q26- Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree*"
      ],
      "metadata": {
        "id": "xszr4ZwNnmUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train regressor with max_depth=5\n",
        "regressor_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "regressor_limited.fit(X_train, y_train)\n",
        "y_pred_limited = regressor_limited.predict(X_test)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "\n",
        "# Train unrestricted regressor\n",
        "regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "regressor_full.fit(X_train, y_train)\n",
        "y_pred_full = regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Print performance comparison\n",
        "print(f\"MSE with max_depth=5: {mse_limited:.4f}\")\n",
        "print(f\"MSE with unrestricted tree: {mse_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "-YRgTqlunp0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q27- Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy*"
      ],
      "metadata": {
        "id": "urPYsWXinu_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train initial tree to get pruning path\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get effective alphas and corresponding total impurities for pruning\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Lists to store results\n",
        "clfs = []\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "# Train trees for each alpha value\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "    train_pred = clf.predict(X_train)\n",
        "    test_pred = clf.predict(X_test)\n",
        "\n",
        "    train_scores.append(accuracy_score(y_train, train_pred))\n",
        "    test_scores.append(accuracy_score(y_test, test_pred))\n",
        "\n",
        "# Plot accuracy vs alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_scores, marker='o', label='Train Accuracy', drawstyle=\"steps-post\")\n",
        "plt.plot(ccp_alphas, test_scores, marker='o', label='Test Accuracy', drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"ccp_alpha (Pruning parameter)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Decision Tree Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0E0sfdgDnzvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q28- Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score*"
      ],
      "metadata": {
        "id": "s5JX4oIBn5D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Generate classification report\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "o6WQ4lt-n8H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q29- Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn*"
      ],
      "metadata": {
        "id": "xCpjSO5boBYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names,\n",
        "            yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - Decision Tree Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U6kJOZMgoE8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q30- Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split."
      ],
      "metadata": {
        "id": "F4WBf6C5oLrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Run grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best hyperparameters and best score\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate best model on test data\n",
        "best_clf = grid_search.best_estimator_\n",
        "test_accuracy = best_clf.score(X_test, y_test)\n",
        "print(f\"Test set accuracy with best parameters: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "-eNa8yFsoRFE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}