{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMv3xm21ibJ/gaHdfAwAbqr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abha989/Python-Assignments/blob/main/Evaluation_Metrics_and_Regression_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgCN_xjS-M7x"
      },
      "outputs": [],
      "source": [
        "Q1- G, What does R-squared represent in a regression model?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''In a regression model, R-squared represents the proportion of the variance in the dependent variable (the outcome) that is explained by the independent\n",
        "variables (the predictors). Essentially, it quantifies how well the regression line fits the observed data points."
      ],
      "metadata": {
        "id": "OOjgWyqAFGAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q2-  What are the assumptions of linear regression?"
      ],
      "metadata": {
        "id": "pApZBn-8FJH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Linear regression relies on several key assumptions for its validity and reliability. These include:\n",
        "linearity, independence, homoscedasticity, normality, and no multicollinearity. Violating these assumptions can lead to inaccurate predictions and biased model\n",
        "results.\n"
      ],
      "metadata": {
        "id": "keyWa8vVFOv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3- What is the difference between R-squared and Adjusted R-squared?"
      ],
      "metadata": {
        "id": "NANpG33hFVSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' R-squared (R²) and adjusted R-squared are both statistical measures used in regression analysis to assess the goodness of fit of a model, but they differ\n",
        "in how they account for the number of predictors in the model. R-squared measures the proportion of variance in the dependent variable that is explained by the\n",
        "independent variables, and it always increases when more predictors are added, even if they don't improve the model's predictive accuracy.\n",
        "Adjusted R-squared, on the other hand, penalizes the model for including unnecessary predictors and increases only if the added predictors significantly\n",
        "improve the model's predictive ability.\n"
      ],
      "metadata": {
        "id": "n0P8-L3tFbJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4- Why do we use Mean Squared Error (MSE)?"
      ],
      "metadata": {
        "id": "0pzfhU7lFiPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Mean squared error (MSE) is widely used in machine learning and statistics to measure the average squared difference between predicted and actual values\n",
        "in regression problems. It helps evaluate model performance and is sensitive to both large and small errors, giving greater weight to larger errors."
      ],
      "metadata": {
        "id": "HEVrv1cZFmYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5- What does an Adjusted R-squared value of 0.85 indicate:"
      ],
      "metadata": {
        "id": "ckQuQwD3Fs3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' An adjusted R-squared value of 0.85 indicates that 85% of the variance in the dependent variable is explained by the independent variables in the model,\n",
        "while also accounting for the number of predictors included. This suggests a good fit, but the adjusted R-squared helps prevent overfitting by penalizing the\n",
        "addition of unnecessary variables.\n"
      ],
      "metadata": {
        "id": "lpSf6SKYFxjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6- How do we check for normality of residuals in linear regression?"
      ],
      "metadata": {
        "id": "GTpS4pk9F3xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' To check for the normality of residuals in linear regression, you can use both graphical and statistical methods.\n",
        "A QQ plot (quantile-quantile plot) is a common graphical method, where you plot the theoretical quantiles of a normal distribution against the observed\n",
        "quantiles of your residuals. If the residuals are normally distributed, the points in the QQ plot will fall approximately on a straight line.\n",
        "Additionally, you can use statistical tests like the Shapiro-Wilk test to formally test for normality."
      ],
      "metadata": {
        "id": "rmqbxxH3F7Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7-  What is multicollinearity, and how does it impact regression?"
      ],
      "metadata": {
        "id": "SvXZ6WlSGFq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Multicollinearity in regression analysis refers to when two or more independent variables in a model are highly correlated,\n",
        "making it difficult to isolate the individual effect of each variable on the dependent variable. This high correlation impacts regression by inflating the\n",
        "standard errors of the coefficients, leading to less precise and unstable coefficient estimates, and potentially making some variables appear statistically\n",
        "insignificant when they should be."
      ],
      "metadata": {
        "id": "2a2W3PLSGQor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8- What is Mean Absolute Error (MAE)?"
      ],
      "metadata": {
        "id": "AyXKkmRCGUrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In statistics, mean absolute error (MAE) is a measure of errors between paired observations expressing the same phenomenon."
      ],
      "metadata": {
        "id": "z_7Y0OK9GYe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q9- What are the benefits of using an ML pipeline?"
      ],
      "metadata": {
        "id": "2YyikFvhGeSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Using a machine learning (ML) pipeline offers several advantages, including improved efficiency, reproducibility, scalability, and easier deployment\n",
        " and collaboration. ML pipelines automate and streamline the process of building, training, and maintaining models, leading to faster development cycles\n",
        " and more reliable results."
      ],
      "metadata": {
        "id": "TTfPvz4HGkMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q10- Why is RMSE considered more interpretable than MSE?"
      ],
      "metadata": {
        "id": "qqhyp3TcGrLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Root Mean Squared Error (RMSE) is often considered more interpretable than Mean Squared Error (MSE) because of the units and scale of the metric:\n",
        "\n",
        "1. Same units as the target variable\n",
        "RMSE is the square root of MSE, so it has the same units as the target variable.\n",
        "\n",
        "MSE, by contrast, is in squared units, which can be unintuitive. For example:\n",
        "\n",
        "If the target is in dollars, RMSE is in dollars (e.g., \"$50 error on average\")\n",
        "\n",
        "MSE is in squared dollars (e.g., \"$2,500²\"), which doesn't have a natural interpretation in most contexts.\n",
        "\n",
        "2. Easier to relate to actual performance\n",
        "Since RMSE is on the same scale as the predictions and actual values, it’s easier for stakeholders to grasp what it means:\n",
        "\n",
        "Saying \"our model has an RMSE of 5°C\" directly communicates average prediction error in degrees.\n",
        "\n",
        "Saying \"our model has an MSE of 25°C²\" is less straightforward.\n",
        "\n",
        "3. Comparing to typical values\n",
        "RMSE allows direct comparison to the standard deviation of the data, helping assess model performance relative to variability in the data."
      ],
      "metadata": {
        "id": "QmmyR9OwGwOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q11- What is pickling in Python, and how is it useful in ML?"
      ],
      "metadata": {
        "id": "ueCvBhtbG9V9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Pickling in Python is the process of serializing Python objects, converting them into a byte stream for storage or transmission. This byte stream\n",
        "can then be used to reconstruct the original object later. In machine learning, pickling is commonly used to save trained models, allowing them to be reloaded\n",
        " and used for predictions without retraining.\n"
      ],
      "metadata": {
        "id": "pHMoAu_aHF2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q12- What does a high R-squared value mean?"
      ],
      "metadata": {
        "id": "tnLF_57xHIvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' A high R-squared value indicates that a statistical model effectively explains the variability in the dependent variable.\n",
        " It means that the independent variables included in the model account for a large proportion of the changes observed in the outcome variable. Essentially,\n",
        " a high R-squared suggests a better fit between the model and the data"
      ],
      "metadata": {
        "id": "S_UJHFg1HMuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q13- What happens if linear regression assumptions are violated?"
      ],
      "metadata": {
        "id": "3hgoZzlxHTlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Violating the assumptions of linear regression can lead to a variety of issues, including unreliable parameter estimates,\n",
        "inaccurate confidence intervals, and misleading statistical conclusions. If the assumptions are violated, the model may produce biased or inefficient results,\n",
        " making it difficult to draw valid inferences or make accurate predictions"
      ],
      "metadata": {
        "id": "cpWIAwA4HX2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q14- How can we address multicollinearity in regression?"
      ],
      "metadata": {
        "id": "5e9wMnwCHfSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' To address multicollinearity in regression analysis, several strategies can be employed, including removing highly correlated predictors,\n",
        "combining them into a single variable, or using regularization techniques like Ridge or Lasso regression. Additionally, increasing the sample size or\n",
        "using dimensionality reduction techniques like Principal Component Analysis (PCA) can help mitigate multicollinearity.\n"
      ],
      "metadata": {
        "id": "dR_wFssXHnBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q15- How can feature selection improve model performance in regression analysis?"
      ],
      "metadata": {
        "id": "L9WMVxHtHqHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''Feature selection improves regression model performance by simplifying the model, reducing overfitting, and enhancing interpretability. By focusing on the\n",
        "most relevant features, it can improve predictive accuracy, reduce training time, and make the model easier to understand.\n"
      ],
      "metadata": {
        "id": "KikpohSoHwAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q16- How is Adjusted R-squared calculated?"
      ],
      "metadata": {
        "id": "VHOQEP1tH0Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' To calculate the adjusted R-squared, use the formula: Adjusted R-squared = 1 - [(1 - R) (n - 1) / (n - k - 1)], where 'n' is the number of data points, 'k'\n",
        " is the number of independent variables, and 'R' is the standard R-squared value. This formula adjusts the R-squared for the number of predictors in the model,\n",
        " ensuring only significant predictors enhance its value.\n"
      ],
      "metadata": {
        "id": "t5Ew4OV-H3rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q17- Why is MSE sensitive to outliers?"
      ],
      "metadata": {
        "id": "_W74FTJkH9xK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Mean Squared Error (MSE) is sensitive to outliers because it squares the differences between predicted and actual values before averaging them.\n",
        " This squaring process amplifies large errors, meaning that a single outlier with a substantial error can have a disproportionately large impact on the\n",
        " overall MSE value, potentially skewing the performance evaluation of a model."
      ],
      "metadata": {
        "id": "Dqg15FwtIBaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q18- What is the role of homoscedasticity in linear regression?"
      ],
      "metadata": {
        "id": "ITNpKzHNIIz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In linear regression, homoscedasticity means that the variance of the error terms (residuals) is constant across all levels of the independent variables.\n",
        " This is a crucial assumption for the validity of many statistical tests and the efficiency of least squares estimation.\n",
        " If this assumption is violated (resulting in heteroscedasticity), the standard errors of the regression coefficients can be biased, leading to unreliable\n",
        " inferences about the model."
      ],
      "metadata": {
        "id": "q1miKPxiIMot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q19- What is Root Mean Squared Error (RMSE)?"
      ],
      "metadata": {
        "id": "R78qSbGBIUUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Root Mean Squared Error (RMSE) is a commonly used metric to measure the accuracy of a model's predictions. It tells you how far off, on average,\n",
        "your model’s predictions are from the actual values—with a strong penalty for larger errors.\n",
        "\n"
      ],
      "metadata": {
        "id": "chChFQpPIX5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q20-  Why is pickling considered risky?"
      ],
      "metadata": {
        "id": "LzNRWI9eIfJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Pickle files can execute arbitrary code during loading (pickle.load()), which poses a serious security threat.\n",
        "\n",
        "If you load a pickle file from an untrusted source, it could execute malicious code (e.g., deleting files, installing malware).\n"
      ],
      "metadata": {
        "id": "E-Kag5QIIjle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q21- What alternatives exist to pickling for saving ML models?"
      ],
      "metadata": {
        "id": "rfsUiYX6JNM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Joblib is an alternative tool to pickle that we can use to save [and load] our models. It's part of SciPy's ecosystem and is much more efficient on objects that carry large NumPy arrays"
      ],
      "metadata": {
        "id": "D08uaW4nJQ9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q22- What is heteroscedasticity, and why is it a problem?"
      ],
      "metadata": {
        "id": "xGtOWNz2JdC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Heteroscedasticity, in the context of regression analysis, is a situation where the variability (or spread) of the dependent variable is not constant across the range of values of the independent variable. Essentially, the errors in the model's predictions have different variances depending on the predictor variable's value. This is a problem because it violates the assumptions of ordinary least squares (OLS) regression, leading to inefficient and potentially misleading estimates of regression coefficients and standard errors,\n",
        "which can affect hypothesis testing."
      ],
      "metadata": {
        "id": "DVrEV1IwJlQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q23- How can interaction terms enhance a regression model's predictive power?"
      ],
      "metadata": {
        "id": "IFK0E6xgJqCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Interaction terms in regression models enhance predictive power by capturing the combined effect of two or more independent variables on the dependent variable. This allows the model to account for how the relationship between a target variable and a predictor variable changes depending on the value of another predictor variable. By acknowledging these complex relationships, interaction terms make the model more flexible and capable of fitting the data more accurately,\n",
        " leading to better predictions."
      ],
      "metadata": {
        "id": "2eKet0NLJw2a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}