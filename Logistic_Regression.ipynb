{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXkp6hIz3OxkQwJ2fmUJOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abha989/Python-Assignments/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HccGyaMMQ5Yh"
      },
      "outputs": [],
      "source": [
        "Q1- What is Logistic Regression, and how does it differ from Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Logistic regression and linear regression are both used in statistics and machine learning for making predictions, but they differ in how they handle the\n",
        " dependent variable and the type of prediction they make. Linear regression predicts a continuous numerical value, while logistic regression predicts a\n",
        " categorical outcome, typically a binary one"
      ],
      "metadata": {
        "id": "LLutOFjJRFuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q2-  What is the mathematical equation of Logistic Regression."
      ],
      "metadata": {
        "id": "no4q9F7sRI1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The mathematical equation for logistic regression models the probability of an event occurring given a set of input features.\n",
        "It uses the logistic function (also known as the sigmoid function) to transform a linear combination of input features into a probability between 0 and 1."
      ],
      "metadata": {
        "id": "kXsYHsR3UEyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3- Why do we use the Sigmoid function in Logistic Regression."
      ],
      "metadata": {
        "id": "VirQgpCjRLtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The sigmoid function is used in logistic regression to convert the linear output of the model into a probability between 0 and 1.\n",
        "This probability represents the likelihood of the input belonging to a specific class, making it suitable for binary classification tasks.\n"
      ],
      "metadata": {
        "id": "ynXYh-vdUKqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4- What is the cost function of Logistic Regression."
      ],
      "metadata": {
        "id": "DJ_Om9nBUQWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The cost function for logistic regression is typically log-loss, also known as binary cross-entropy, which measures the difference between the predicted\n",
        "probabilities and the actual class labels. It penalizes incorrect predictions more heavily as they deviate from the true labels.\n"
      ],
      "metadata": {
        "id": "g2iVRqFeUTdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5- What is Regularization in Logistic Regression? Why is it needed."
      ],
      "metadata": {
        "id": "rVppOiRPUXui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Regularization in logistic regression is a technique that adds a penalty term to the loss function during model training to prevent overfitting and improve generalization performance. It helps the model learn simpler relationships between the features and the target variable, making it more robust and accurate on unseen data.\n",
        "Why Regularization is Needed:\n",
        "Overfitting:\n",
        "Logistic regression, like other machine learning models, can overfit the training data, meaning it learns the noise and specific details of the training set, leading to poor performance on new, unseen data.\n",
        "High Variance:\n",
        "Overfitting is often associated with high variance, where the model's predictions are highly sensitive to small changes in the training data.\n",
        "Generalization:\n",
        "Regularization helps to reduce the variance and improve the model's ability to generalize to new data by discouraging the model from assigning excessive importance to any single feature.\n",
        "Simpler Models:\n",
        "By penalizing large coefficients, regularization encourages the model to learn simpler relationships, which are less likely to overfit the training data."
      ],
      "metadata": {
        "id": "OR-0TsEwUfRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6- Explain the difference between Lasso, Ridge, and Elastic Net regressionC"
      ],
      "metadata": {
        "id": "SFMbrRftUiu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Lasso, Ridge, and Elastic Net regressions are all regularization techniques used to prevent overfitting in linear models. Ridge regression uses L2\n",
        "regularization, which penalizes the sum of the squared coefficients, shrinking them towards zero but never eliminating them. Lasso regression uses L1\n",
        "regularization, penalizing the sum of the absolute values of the coefficients, and can set some coefficients exactly to zero, effectively performing feature\n",
        "selection. Elastic Net combines both L1 and L2 penalties, offering a balance between feature selection and coefficient shrinkage."
      ],
      "metadata": {
        "id": "EtEi6v5gUmBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7- When should we use Elastic Net instead of Lasso or Ridge."
      ],
      "metadata": {
        "id": "_vaLE-8QUuM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Elastic Net should be used when you have many correlated features and you need a balance between feature selection and shrinkage.\n",
        "It's especially useful when you have high-dimensional data and a small number of samples. Unlike Lasso, which can randomly remove one feature from a group of\n",
        "correlated features, Elastic Net can select groups of correlated features.\n"
      ],
      "metadata": {
        "id": "d7OM2KWbUyLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8- What is the impact of the regularization parameter (λ) in Logistic Regression."
      ],
      "metadata": {
        "id": "CGTH6eQlU4PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In Logistic Regression, the regularization parameter (λ) controls the strength of the penalty term, influencing the model's complexity and preventing\n",
        "overfitting. A higher λ value leads to stronger regularization, shrinking coefficients towards zero and creating a simpler model. This helps the model\n",
        "generalize better to new, unseen data."
      ],
      "metadata": {
        "id": "mQ9QwLaPU7oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q9- What are the key assumptions of Logistic Regression."
      ],
      "metadata": {
        "id": "7VzpZkvZVCp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Logistic regression makes several key assumptions to ensure reliable and valid results. These include linearity of the log-odds, independence of\n",
        "observations, absence of multicollinearity, and a large enough sample size. Additionally, the model should not be unduly influenced by outliers, and the outcome\n",
        " variable should be binary or categorical.\n"
      ],
      "metadata": {
        "id": "ZJnAgpeGVFqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q10- What are some alternatives to Logistic Regression for classification tasks."
      ],
      "metadata": {
        "id": "h4jRrlfhVL6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Several algorithms offer alternatives to Logistic Regression for classification tasks, including Support Vector Machines (SVMs), Decision Trees, Random\n",
        "Forests, and Naive Bayes. These methods can be particularly useful when dealing with non-linear relationships or when the dataset is large or complex"
      ],
      "metadata": {
        "id": "PuHoT5xMVOvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q11- What are Classification Evaluation Metrics."
      ],
      "metadata": {
        "id": "yGiT8S30VVKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Classification evaluation metrics are used to assess how well a machine learning model performs in predicting the correct class for a given input.\n",
        "These metrics provide insights into a model's accuracy, precision, recall, and other aspects of its predictive ability. Common metrics include accuracy,\n",
        "precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC)."
      ],
      "metadata": {
        "id": "58PkTYM3VX1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q12- C How does class imbalance affect Logistic Regression."
      ],
      "metadata": {
        "id": "RTEfyHjfVdgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Class imbalance, where one class has significantly more observations than the other, can negatively impact logistic regression models.\n",
        "This leads to biased parameter estimates, poor performance for the minority class, and reduced generalization ability. Traditional logistic regression may\n",
        "favor the majority class, neglecting the minority class"
      ],
      "metadata": {
        "id": "i5yxKl8IVgfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q13- What is Hyperparameter Tuning in Logistic Regression."
      ],
      "metadata": {
        "id": "TzDYhmJRVmLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Hyperparameter tuning in logistic regression is the process of optimizing the performance of a logistic regression model by adjusting its non-learned\n",
        "parameters, also known as hyperparameters. These parameters are set before training and influence how the model learns from the data.\n"
      ],
      "metadata": {
        "id": "cWtrs8WkVpYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q14- What are different solvers in Logistic Regression? Which one should be used."
      ],
      "metadata": {
        "id": "wiujyX6SVu91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In Logistic Regression, different solvers are optimization algorithms used to find the best model parameters. The choice of solver depends on the size and characteristics of your dataset, as well as your specific needs regarding computational efficiency, regularization, and multiclass classification. Here's a breakdown of some common solvers and when to use them:\n",
        "liblinear:\n",
        "This solver is efficient for small datasets, especially when sparse (many zero values). It's also suitable for binary classification problems and supports L1 and L2 regularization. However, it can't handle multiclass problems directly.\n",
        "lbfgs (Limited-memory Broyden–Fletcher–Goldfarb–Shanno):\n",
        "This is a good general-purpose solver, particularly for medium-sized datasets. It's computationally efficient and works well with dense datasets. It supports L2 regularization but not L1.\n",
        "sag (Stochastic Average Gradient):\n",
        "This solver is designed for large datasets. It uses stochastic gradient descent to update the parameters efficiently, making it suitable for large-scale problems. It supports L2 regularization.\n",
        "saga (Stochastic Accelerated Gradient):\n",
        "Similar to SAG, but it's a variant that can handle L1 regularization as well. This makes it a good choice for very large datasets or when L1 regularization is needed.\n",
        "newton-cg (Newton's Method with Conjugate Gradient):\n",
        "This solver computes the full Hessian matrix, which can be computationally expensive for high-dimensional datasets. However, it can be efficient for datasets where the number of samples is much larger than the number of features. It supports L2 regularization.\n",
        "srrg (Stochastic Recursive Regularized Gradient):\n",
        "This solver is similar to SAG and SAGA but uses a different update mechanism. It is also designed for large datasets and can handle L1 and L2 regularization.\n",
        "Which solver to use:\n",
        "For small datasets or sparse data with binary classification: Use liblinear.\n",
        "For medium-sized datasets or general-purpose logistic regression: Use lbfgs.\n",
        "For large datasets: Use sag or saga.\n",
        "For multiclass problems (except liblinear): Any solver except liblinear can be used. However, liblinear can be used with a one-versus-rest strategy.\n",
        "For L1 regularization: Use saga or srrg.\n"
      ],
      "metadata": {
        "id": "xopDRvTWVyWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q15- How is Logistic Regression extended for multiclass classification."
      ],
      "metadata": {
        "id": "PCt5Oth2V6c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Logistic Regression is extended for multiclass classification using techniques like One-vs-Rest (OvR) or One-vs-One. OvR trains a separate binary logistic\n",
        "regression model for each class, treating it against all other classes, while One-vs-One trains a classifier for each pair of classes. One-vs-All is also\n",
        "known as One-vs-Rest and trains one model for each class, predicting the class with the highest probability.\n"
      ],
      "metadata": {
        "id": "xrQfUCW1V-uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q16- What are the advantages and disadvantages of Logistic Regression."
      ],
      "metadata": {
        "id": "6uQb1gOQWFFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''  Advantages\n",
        "\n",
        "1- Easy to implement and interpret yet efficient in training\n",
        "2- The predicted parameters give inference about the importance of each feature\n",
        "3- Performs well on low-dimensional data.\n",
        "4- Very efficient when the dataset has features that are linearly separable.\n",
        "5- Outputs well-calibrated probabilities along with classification results.\n",
        "\n",
        "Disadvantage\n",
        "\n",
        "1- Overfits on high dimensional data\n",
        "2- Non linear problems can't be solved with logistic regression since it has a linear decision surface\n",
        "3- \tAssumes linearity between dependent and independent variables.\n",
        "4- Fails to capture complex relationships.\n",
        "5- Only important and relevant features should be used otherwise model's predictive value will degrade."
      ],
      "metadata": {
        "id": "E-QPiwIBWIZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q17- What are some use cases of Logistic Regression."
      ],
      "metadata": {
        "id": "aSEdHkJxWuQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Logistic regression, a statistical method used for binary classification, finds applications across various fields like healthcare, finance, marketing, and more. It predicts the probability of an event happening (e.g., yes/no, 1/0) based on input variables.\n",
        "Here are some key use cases of logistic regression:\n",
        "1. Healthcare:\n",
        "Predicting disease risk:\n",
        "Estimating the likelihood of a patient developing a specific disease based on factors like family history, lifestyle, and medical history.\n",
        "Medical diagnosis:\n",
        "Classifying whether a patient has a particular condition based on test results and symptoms.\n",
        "Patient mortality prediction:\n",
        "Assessing the probability of a patient's survival based on their condition and other factors.\n",
        "2. Finance:\n",
        "Fraud detection:\n",
        "Identifying fraudulent transactions based on transaction patterns, user behavior, and other relevant data.\n",
        "Credit risk assessment:\n",
        "Evaluating the probability of a loan applicant defaulting on their loan based on their credit history and other factors.\n",
        "Insurance risk assessment:\n",
        "Predicting the likelihood of an insurance claim based on factors like age, driving history, and location.\n",
        "3. Marketing:\n",
        "Click-through rate prediction:\n",
        "Estimating the probability of a user clicking on an advertisement based on their demographics, browsing history, and other factors.\n",
        "Customer churn prediction:\n",
        "Identifying customers at risk of leaving a business based on their engagement patterns and other behavioral data.\n",
        "Targeted advertising:\n",
        "Identifying which customers are most likely to respond to a specific marketing campaign based on their demographics and online behavior.\n",
        "4. Other Industries:\n",
        "Email spam detection:\n",
        "Identifying spam emails based on keywords, sender information, and other features.\n",
        "Predicting part failure in manufacturing:\n",
        "Estimating the probability of a machine part failing based on its age, usage, and environmental conditions.\n",
        "Disaster management:\n",
        "Predicting the likelihood of certain behaviors during disasters (e.g., evacuation) based on factors like location, socioeconomic status, and disaster type."
      ],
      "metadata": {
        "id": "UJELGDEmWxdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q18- What is the difference between Softmax Regression and Logistic Regression."
      ],
      "metadata": {
        "id": "U4ZqA80KWxbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In logistic regression we assumed that the labels were binary: y(i)∈{0,1} . We used such a classifier to distinguish between two kinds of hand-written digits.\n",
        " Softmax regression allows us to handle y(i)∈{1,…,K} where K is the number of classes."
      ],
      "metadata": {
        "id": "82dWpRUDW-GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q19- How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification."
      ],
      "metadata": {
        "id": "XiA_sdwDXE7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The choice between One-vs-Rest (OvR) and Softmax for multiclass classification depends on the specific problem and the characteristics of the data.\n",
        "OvR is often preferred for its simplicity and ability to handle large datasets, while Softmax offers a unified approach that can be more effective when class\n",
        "relationships are important.\n"
      ],
      "metadata": {
        "id": "jBXHR8H0XIIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q20- How do we interpret coefficients in Logistic Regression?"
      ],
      "metadata": {
        "id": "rERTYowEXQas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In logistic regression, coefficients are interpreted by considering their sign and magnitude, and by exponentiating them to find odds ratios.\n",
        " A positive coefficient means the log-odds of the event increases with a one-unit increase in the predictor variable, while a negative coefficient indicates a\n",
        " decrease. Exponentiating the coefficient (e^coefficient) yields the odds ratio, which represents how much the odds of the event change for a one-unit change in\n",
        "  the predictor.\n"
      ],
      "metadata": {
        "id": "Nh2U1YduXT2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRACTICAL"
      ],
      "metadata": {
        "id": "1-ofrlU-Xa2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 C Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracyC"
      ],
      "metadata": {
        "id": "0-fGVrifXcDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions and calculate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "394gPfAbX1dL",
        "outputId": "54ec3604-ab79-4082-b22b-bbe50d1163ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q2- Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracyC"
      ],
      "metadata": {
        "id": "BLxJQiGkX4xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Filter for binary classification (since L1 in LogisticRegression works best with binary)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Logistic Regression with L1 regularization (use solver that supports L1)\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear')\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy\n",
        "print(f\"Model Accuracy with L1 Regularization: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT08Ypf6X8qw",
        "outputId": "401af9b8-880a-460a-f511-ac21ebdaea70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 Regularization: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q3- Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC"
      ],
      "metadata": {
        "id": "ladtrTrhYCKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Logistic Regression with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='auto', max_iter=200)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy and coefficients\n",
        "print(f\"Model Accuracy with L2 Regularization: {accuracy:.2f}\")\n",
        "print(\"Model Coefficients (per class):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv8TLXHdYFfq",
        "outputId": "c95e2c21-4f90-4044-b234-d4992b742335"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 Regularization: 1.00\n",
            "Model Coefficients (per class):\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q4- Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')C"
      ],
      "metadata": {
        "id": "9nRobb15YQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, reduce to binary classification (only 2 classes)\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Logistic Regression with Elastic Net Regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',         # 'saga' is required for elasticnet\n",
        "    l1_ratio=0.5,          # Balance between L1 and L2 (0.5 means equal weight)\n",
        "    max_iter=1000\n",
        ")\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Output results\n",
        "print(f\"Model Accuracy with Elastic Net Regularization: {accuracy:.2f}\")\n",
        "print(\"Model Coefficients:\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXmq2lHlYRS8",
        "outputId": "d610051a-ec72-4bd2-8a80-37dfc4f9fb81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net Regularization: 1.00\n",
            "Model Coefficients:\n",
            "[[ 0.         -0.95303044  2.54041252  0.57106192]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q5- Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr"
      ],
      "metadata": {
        "id": "cBb9bFRRYc-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (3 classes: 0, 1, 2)\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Logistic Regression with One-vs-Rest (OvR) strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=200)\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"Model Accuracy (OvR Multiclass): {accuracy:.2f}\")\n",
        "print(\"Model Coefficients (per class):\")\n",
        "print(model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQcMOmzgYgD_",
        "outputId": "f3d621ad-a3b7-43c7-e0cc-e6a11e7ef355"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy (OvR Multiclass): 0.97\n",
            "Model Coefficients (per class):\n",
            "[[-0.42762216  0.88771927 -2.21471658 -0.91610036]\n",
            " [-0.03387836 -2.0442989   0.54266011 -1.0179372 ]\n",
            " [-0.38904645 -0.62147609  2.7762982   2.09067085]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q6- Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracyC"
      ],
      "metadata": {
        "id": "xMBZ6sb1YhoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# For simplicity, convert to binary classification\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Step 2: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both l1 and l2 penalties\n",
        "}\n",
        "\n",
        "# Step 4: Setup GridSearchCV\n",
        "grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Step 5: Fit GridSearchCV\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict and evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print best parameters and accuracy\n",
        "print(\"Best Parameters Found by GridSearchCV:\")\n",
        "print(grid.best_params_)\n",
        "print(f\"Test Accuracy with Best Parameters: {test_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBnNy8MdYk-i",
        "outputId": "43f0308d-de67-432f-cd7c-4e9155efb1cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found by GridSearchCV:\n",
            "{'C': 0.01, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Test Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q7- Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracyC"
      ],
      "metadata": {
        "id": "kyt9AqfJYr7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Define the model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Step 3: Setup Stratified K-Fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Evaluate model using cross_val_score\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Accuracy scores for each fold: {scores}\")\n",
        "print(f\"Average Accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivt_bDPxYunF",
        "outputId": "04f125df-7f79-4f90-b36a-fbaedbb90974"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q8- Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "lMBafQszY0WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os # Import the os module\n",
        "\n",
        "# Step 1: Load dataset from CSV\n",
        "# Replace 'your_file.csv' with the actual path to your CSV file\n",
        "# If you don't have a CSV, you can create a dummy one for demonstration or skip this cell.\n",
        "\n",
        "# --- Suggested Change: Create a dummy CSV if it doesn't exist for demonstration ---\n",
        "csv_file_path = 'your_file.csv'\n",
        "\n",
        "if not os.path.exists(csv_file_path):\n",
        "    print(f\"Creating a dummy CSV file at: {csv_file_path}\")\n",
        "    # Create a simple DataFrame with some data\n",
        "    dummy_data = {\n",
        "        'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "        'feature2': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        'target': [0, 0, 0, 0, 1, 1, 1, 1, 1, 1] # Binary target\n",
        "    }\n",
        "    dummy_df = pd.DataFrame(dummy_data)\n",
        "    dummy_df.to_csv(csv_file_path, index=False)\n",
        "    print(\"Dummy CSV created.\")\n",
        "# --- End of Suggested Change ---\n",
        "\n",
        "\n",
        "try:\n",
        "    data = pd.read_csv(csv_file_path)\n",
        "    print(f\"Successfully loaded data from {csv_file_path}\")\n",
        "\n",
        "    # Step 2: Separate features and target\n",
        "    # Replace 'target' with the actual name of your target column\n",
        "    # Ensure the column names exist in your CSV\n",
        "    if 'target' in data.columns:\n",
        "        X = data.drop('target', axis=1)\n",
        "        y = data['target']\n",
        "\n",
        "        # Check if there are enough samples and features\n",
        "        if X.shape[0] > 1 and X.shape[1] > 0 and y.nunique() > 1:\n",
        "\n",
        "            # Step 3: Split into training and testing sets\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "            # Step 4: Train Logistic Regression model\n",
        "            # Adjust max_iter if convergence warning occurs\n",
        "            model = LogisticRegression(max_iter=1000)\n",
        "            model.fit(X_train, y_train)\n",
        "\n",
        "            # Step 5: Predict and evaluate\n",
        "            y_pred = model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Step 6: Print accuracy\n",
        "            print(f\"Logistic Regression Accuracy: {accuracy:.2f}\")\n",
        "        else:\n",
        "            print(\"Dataset is too small or does not have enough distinct classes for meaningful training.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Error: 'target' column not found in the CSV file. Please check the column names.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNcoWP4bZC8g",
        "outputId": "2265e217-a2e6-4e6f-bf70-aa112616386c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a dummy CSV file at: your_file.csv\n",
            "Dummy CSV created.\n",
            "Successfully loaded data from your_file.csv\n",
            "Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q9- Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracyM"
      ],
      "metadata": {
        "id": "RSv0VQc0ZEnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Step 1: Load dataset (you can replace this with pd.read_csv('your_file.csv'))\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the parameter distribution\n",
        "param_dist = {\n",
        "    'C': loguniform(0.001, 100),  # Continuous range for C\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']  # These solvers support both l1 and l2\n",
        "}\n",
        "\n",
        "# Step 4: Initialize Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Step 5: Setup RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Fit on training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Evaluate on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 8: Print results\n",
        "print(\"Best Parameters Found by RandomizedSearchCV:\")\n",
        "print(random_search.best_params_)\n",
        "print(f\"Test Accuracy with Best Parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uXy38PaZJwY",
        "outputId": "0f7b2a2c-97d6-43ce-8d60-072756d16cef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found by RandomizedSearchCV:\n",
            "{'C': np.float64(14.528246637516036), 'penalty': 'l2', 'solver': 'saga'}\n",
            "Test Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q10- Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy"
      ],
      "metadata": {
        "id": "JmsI7wOEZQv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Initialize One-vs-One classifier with Logistic Regression\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "\n",
        "# Step 4: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_FKIFSsZT-m",
        "outputId": "6b85c90a-dc2b-42c0-cb80-cc6c1480c3cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q11- Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classificationM"
      ],
      "metadata": {
        "id": "bLycyCH_ZZuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 7: Visualize confusion matrix\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "E9FIk0m0ZdUA",
        "outputId": "3df311b2-5bfc-4897-b5bc-91fab9adec68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.96\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q12- Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-ScoreM"
      ],
      "metadata": {
        "id": "17G3VD1dZidt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load binary classification dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n",
        "print(f\"F1-Score:  {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "Xjgt3IqpZmOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q13- Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performanceM"
      ],
      "metadata": {
        "id": "bw64t0SmZrxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Generate imbalanced dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Original class distribution: {Counter(y)}\")\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression without class weights\n",
        "model_no_weights = LogisticRegression(max_iter=1000)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"\\nWithout class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Step 4: Train Logistic Regression with class weights ('balanced')\n",
        "model_weights = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_weights.fit(X_train, y_train)\n",
        "y_pred_weights = model_weights.predict(X_test)\n",
        "\n",
        "print(\"\\nWith class weights (balanced):\")\n",
        "print(classification_report(y_test, y_pred_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "janeJWjWZvMb",
        "outputId": "3e50af2f-47d7-4524-b1d6-773b375d2487"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original class distribution: Counter({np.int64(0): 900, np.int64(1): 100})\n",
            "\n",
            "Without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96       185\n",
            "           1       0.50      0.40      0.44        15\n",
            "\n",
            "    accuracy                           0.93       200\n",
            "   macro avg       0.73      0.68      0.70       200\n",
            "weighted avg       0.92      0.93      0.92       200\n",
            "\n",
            "\n",
            "With class weights (balanced):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.84      0.90       185\n",
            "           1       0.25      0.67      0.36        15\n",
            "\n",
            "    accuracy                           0.82       200\n",
            "   macro avg       0.61      0.75      0.63       200\n",
            "weighted avg       0.91      0.82      0.86       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q14- Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performanceM"
      ],
      "metadata": {
        "id": "CJKeM6NgZ0-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "# If you don't have the file, you can download from https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\n",
        "titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
        "df = pd.read_csv(titanic_url)\n",
        "\n",
        "# Step 2: Select features and target\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# Step 3: Define preprocessing for numeric and categorical features\n",
        "numeric_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 4: Create pipeline with preprocessing and logistic regression\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Step 5: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoRJpCl-Z5pP",
        "outputId": "6e3c3d3f-9e50-4163-9077-40456a491529"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.81\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.86      0.84       105\n",
            "           1       0.79      0.74      0.76        74\n",
            "\n",
            "    accuracy                           0.81       179\n",
            "   macro avg       0.81      0.80      0.80       179\n",
            "weighted avg       0.81      0.81      0.81       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q 15- Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling"
      ],
      "metadata": {
        "id": "nzofOGM4aBOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Logistic Regression WITHOUT scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# 2. Logistic Regression WITH StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaling = LogisticRegression(max_iter=1000)\n",
        "model_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = model_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(f\"Accuracy WITHOUT scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy WITH scaling:    {accuracy_scaling:.2f}\")\n"
      ],
      "metadata": {
        "id": "nurl346waDnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q16- Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC scoreM"
      ],
      "metadata": {
        "id": "zZqN2nDWaKjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities for positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n"
      ],
      "metadata": {
        "id": "tm5HBvnGaMZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q17- Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracyM"
      ],
      "metadata": {
        "id": "hGnV6MbuaRq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression with custom C (regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "razo_gNtaYT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q18- Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficientsM"
      ],
      "metadata": {
        "id": "ncWtwNt1agUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Create a DataFrame of features and their coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients,\n",
        "    'Absolute Coefficient': np.abs(coefficients)\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient descending\n",
        "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "print(\"Feature importance based on model coefficients:\")\n",
        "print(feature_importance[['Feature', 'Coefficient']])\n"
      ],
      "metadata": {
        "id": "R40SClzpajOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q19- Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa\n",
        "ScoreM"
      ],
      "metadata": {
        "id": "B0mn-0wnamVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Cohen's Kappa Score: {kappa:.3f}\")\n"
      ],
      "metadata": {
        "id": "Wk2puYcRaqTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q20- Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio:"
      ],
      "metadata": {
        "id": "fWwFQwrpau9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Compute average precision score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Logistic Regression (AP = {avg_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "E72Z58Ata0vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q 21- Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracyM"
      ],
      "metadata": {
        "id": "J42zZbADa3kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of solvers to compare\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracy scores\n",
        "accuracies = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Create Logistic Regression model with specified solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Store accuracy\n",
        "    accuracies[solver] = accuracy\n",
        "\n",
        "# Print accuracy results\n",
        "print(\"Accuracy comparison for different solvers:\")\n",
        "for solver, acc in accuracies.items():\n",
        "    print(f\"Solver: {solver:8} --> Accuracy: {acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "UhOI7NU6a7KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q22- Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)M"
      ],
      "metadata": {
        "id": "oRnvNvMOa_gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.3f}\")\n"
      ],
      "metadata": {
        "id": "ljlWyFLEbC2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q23- Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scalingM"
      ],
      "metadata": {
        "id": "AuDG01N0bHNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# 2. Train Logistic Regression on standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print accuracy comparison\n",
        "print(f\"Accuracy on raw data:         {accuracy_raw:.3f}\")\n",
        "print(f\"Accuracy on standardized data: {accuracy_scaled:.3f}\")\n"
      ],
      "metadata": {
        "id": "3unAyC7TbJxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q 24- Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validationM"
      ],
      "metadata": {
        "id": "v-YokDMTbQJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Define parameter grid for C (regularization strength)\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 5, 10, 100]}\n",
        "\n",
        "# Setup GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter and best score from CV\n",
        "best_C = grid_search.best_params_['C']\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Best C found by cross-validation: {best_C}\")\n",
        "print(f\"Cross-validated accuracy: {best_score:.3f}\")\n",
        "print(f\"Test set accuracy with best C: {test_accuracy:.3f}\")\n"
      ],
      "metadata": {
        "id": "CMkKPiO6bV4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q25- Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "orMI5rWybdwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model_filename = 'logistic_model.joblib'\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Model saved to {model_filename}\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"Model loaded from file.\")\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.3f}\")\n"
      ],
      "metadata": {
        "id": "U-lRX1OSbg6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}