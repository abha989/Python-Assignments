{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMVDFafI09ZArhKDGK59u3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abha989/Python-Assignments/blob/main/Feature_Engineering_assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYaLIw67QyOd"
      },
      "outputs": [],
      "source": [
        "Q1- What is a parameter?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' a parameter is an internal variable within a model that is learned from the data during training, and its values are adjusted to improve the model's\n",
        " performance hese parameters are not set manually beforehand but are determined by the model itself during the learning process."
      ],
      "metadata": {
        "id": "eqo1HJJYREoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q2- What is correlation?"
      ],
      "metadata": {
        "id": "h6NLskNZTH2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.\n",
        "Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it usually refers to the degree to which a pair of\n",
        "variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring,\n",
        "and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the demand curve."
      ],
      "metadata": {
        "id": "2pEIrUmbTMM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q3- Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "SJHR1sdHTXoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Machine Learning (ML) is a branch of artificial intelligence that allows systems to learn from data and improve their performance without being explicitly\n",
        "programmed. It involves using algorithms to analyze data, identify patterns, and make predictions or decisions. The core components of machine learning include\n",
        " data, algorithms, models, and predictions"
      ],
      "metadata": {
        "id": "2W2aIim2T9V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q4- How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "VCFBJlRyUYSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' A lower loss value generally indicates a better-performing model, as it signifies that the model's predictions are closer to the actual values.\n",
        "The loss function quantifies the difference between predicted and actual values, and minimizing this loss is the goal of training a model.\n"
      ],
      "metadata": {
        "id": "H2MNFtnaUcI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q5- What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "dhz8ni15U15Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In data analysis, variables are classified as either continuous or categorical. Continuous variables can take on any value within a given range,\n",
        "while categorical variables represent distinct groups or categories.\n"
      ],
      "metadata": {
        "id": "Z0UQOtxXU32W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q6- How do we handle categorical variables in Machine Learning? What are the common t\n",
        "echniques?"
      ],
      "metadata": {
        "id": "55yn-ikVVF2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Handling categorical variables in machine learning involves converting them into a numerical format that algorithms can understand.\n",
        "Common techniques include label encoding, one-hot encoding, ordinal encoding, and frequency encoding, each suited for different types of categorical data.\n",
        "The best approach depends on the nature of the data and the specific requirements of the model"
      ],
      "metadata": {
        "id": "2al2OPKIVKY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q7- What do you mean by training and testing a dataset?"
      ],
      "metadata": {
        "id": "hV9q1tBPVXLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' In machine learning, a dataset is typically split into training, validation, and testing sets. Training data is used to teach the model,\n",
        " validation data helps fine-tune the model, and testing data evaluates its final performance on unseen data"
      ],
      "metadata": {
        "id": "QPCYccVKVat2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q8-What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "S27458beZuyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to transform raw data into a format suitable\n",
        "for machine learning models. It encompasses various techniques for data scaling, normalization, encoding, and imputation, which are crucial steps in preparing\n",
        "data for analysis and modeling."
      ],
      "metadata": {
        "id": "MXCVT-ZzZArh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q9- What is a Test set?"
      ],
      "metadata": {
        "id": "J1ShDgw3aDCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' a test set is a portion of the dataset used to evaluate the performance of a model that has been trained on a separate dataset, called the training set.\n",
        "It's crucial for understanding how well the model generalizes to unseen data and for comparing the performance of different models."
      ],
      "metadata": {
        "id": "o86ix_JpaHK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q10- How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?"
      ],
      "metadata": {
        "id": "cYz36ejNaPQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''To split data for model fitting in Python, you can use the train_test_split function from the sklearn library. This function divides your dataset into training and testing sets, which are then used to train and evaluate your model, respectively. A typical approach to a machine learning problem involves data collection, preparation, model selection, training, evaluation, and prediction.\n",
        "1. Splitting Data in Python:\n",
        "Use sklearn.model_selection.train_test_split: This function from the scikit-learn library is specifically designed for splitting data into training and testing sets.\n",
        "Define test_size: Specify the proportion of the dataset to be used for testing (e.g., 0.2 for 20% or 0.3 for 30%).\n",
        "Optional: random_state: For reproducibility, set random_state to a specific integer value."
      ],
      "metadata": {
        "id": "1ogh9j_paSz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q11 - Why do we have to perform EDA before fitting a model to the data?"
      ],
      "metadata": {
        "id": "Qvi7iE8cft1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons. It allows you to thoroughly understand the data's characteristics,\n",
        "identify potential problems, and gain insights that inform feature engineering, model selection, and evaluation. By understanding the data beforehand,\n",
        "you can improve model reliability and avoid pitfalls like data leakage"
      ],
      "metadata": {
        "id": "_xM51G0VfzLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q12- What is correlation?"
      ],
      "metadata": {
        "id": "A1izZZBFgLeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' correlation or dependence is any statistical relationship, whether causal or not, between two random variables or bivariate data.\n",
        "Although in the broadest sense, \"correlation\" may indicate any type of association, in statistics it usually refers to the degree to which a pair of\n",
        "variables are linearly related. Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring,\n",
        "and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the demand curve."
      ],
      "metadata": {
        "id": "ZgvYxJD8gQuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q13- What does negative correlation mean?"
      ],
      "metadata": {
        "id": "I3LPnmkcgTcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' A negative correlation means that two variables move in opposite directions. As one variable increases, the other tends to decrease, and vice versa.\n",
        "This is sometimes referred to as an inverse correlation"
      ],
      "metadata": {
        "id": "yjr8vxD0gWqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q14- How can you find correlation between variables in Python?"
      ],
      "metadata": {
        "id": "rs8VSYsKgjpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' To find the correlation between variables in Python, several methods can be employed using libraries like NumPy, pandas, and SciPy.\n",
        "The correlation coefficient, ranging from -1 to 1, measures the strength and direction of a linear relationship between two variables.\n",
        "A value close to 1 indicates a strong positive correlation, -1 a strong negative correlation, and 0 suggests no linear correlation."
      ],
      "metadata": {
        "id": "e3tFBLbqgnem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q15- What is causation? Explain difference between correlation and causation with an example?"
      ],
      "metadata": {
        "id": "y9RAFjKaiNO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Causation means one event directly results in another. Correlation, on the other hand, means two events are related but one doesn't necessarily cause the other.\n",
        " A classic example to illustrate this is ice cream sales and shark attacks. While both tend to increase during the summer, this is due to warmer weather,\n",
        " not ice cream causing shark attacks."
      ],
      "metadata": {
        "id": "2FGMTzAPieiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q16- What is an Optimizer? What are different types of optimizers? Explain each with an example."
      ],
      "metadata": {
        "id": "vSTdQtDbiplK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' An optimizer is an algorithm that adjusts parameters (like weights and biases) of a machine learning model to minimize a loss function, improving its accuracy. Different types of optimizers use various strategies to achieve this, each with its strengths and weaknesses.\n",
        "Types of Optimizers:\n",
        "1. Gradient Descent (GD):\n",
        "A fundamental optimization algorithm that iteratively refines model parameters by moving in the direction of the negative gradient of the loss function.\n",
        "Example: In a linear regression model, GD can adjust the weights of the linear equation to minimize the difference between predicted and actual values.\n",
        "2. Stochastic Gradient Descent (SGD):\n",
        "A variant of GD that updates model parameters based on the gradient calculated from a single training example (or a small batch) at each iteration.\n",
        "Example: In image classification, SGD can update the weights of a convolutional neural network based on the error of a single image.\n",
        "3. Momentum:\n",
        "An extension of SGD that incorporates a momentum term to accelerate convergence and overcome local minima.\n",
        "Example: In natural language processing, momentum can help a recurrent neural network learn more quickly and effectively.\n",
        "4. Nesterov Accelerated Gradient (NAG):\n",
        "A variant of SGD with momentum that uses a \"lookahead\" strategy, making parameter updates more efficient.\n",
        "Example: NAG can be used to train a deep learning model for image recognition, potentially leading to faster convergence and better performance.\n",
        "5. AdaGrad:\n",
        "An adaptive learning rate algorithm that adjusts the learning rate for each parameter based on the historical gradients.\n",
        "Example: In a sparse matrix factorization model, AdaGrad can adapt the learning rate for different features, leading to better performance on sparse data.\n",
        "6. RMSProp:\n",
        "Another adaptive learning rate algorithm that uses the root mean squared error (RMS) of the gradients to adjust the learning rate.\n",
        "Example: RMSProp can be used to train a recurrent neural network for time series forecasting, adapting the learning rate to the fluctuations in the data.\n",
        "7. Adam (Adaptive Moment Estimation):\n",
        "A popular optimizer that combines the advantages of AdaGrad and RMSProp, using adaptive learning rates based on both the first and second moments of the gradients.\n",
        "Example: Adam can be used to train a deep learning model for speech recognition, providing a good balance between convergence speed and stability.\n",
        "8. Nadam (Nesterov-accelerated Adaptive Moment Estimation):\n",
        "A variant of Adam that combines the advantages of NAG and Adam, providing even faster convergence in some cases.\n",
        "Example: Nadam can be used to train a deep convolutional neural network for image recognition, potentially leading to faster convergence and better performance on complex tasks"
      ],
      "metadata": {
        "id": "x2LaTwrSiuN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q17- What is sklearn.linear_model ?"
      ],
      "metadata": {
        "id": "nNZNz9WRi6sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' sklearn.linear_model is a module in the scikit-learn (sklearn) library that implements various linear models for regression and classification tasks.\n",
        "These models assume a linear relationship between the input features and the target variable. It provides tools for fitting linear models to data,\n",
        " making predictions, and evaluating model performance."
      ],
      "metadata": {
        "id": "O0DcyjQxjA5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q18- What does model.fit() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "32C5okrMjJ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' The model.fit() function in Keras trains a neural network model using provided training data. It adjusts the model's weights to minimize the loss function, effectively learning patterns from the data. The function iterates over the dataset in batches for a specified number of epochs.\n",
        "The required arguments for model.fit() are:\n",
        "x: Training data, which can be a NumPy array, a list of arrays (for multi-input models), a dataset, or a generator.\n",
        "y: Target data, corresponding to the training data, used for supervised learning."
      ],
      "metadata": {
        "id": "qeUaKJFmjObg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q19- What does model.predict() do? What arguments must be given?"
      ],
      "metadata": {
        "id": "5SRMz7rbjZe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' model. predict() is used to generate predictions from the trained model based on new input data.\n",
        "It does not require true labels and does not compute any metrics.\n",
        "\n",
        "x: Training data, which can be a NumPy array, a list of arrays (for multi-input models), a dataset, or a generator.\n",
        "y: Target data, corresponding to the training data, used for supervised learning."
      ],
      "metadata": {
        "id": "jobIX2usjdcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q20- What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "RnZDwC6jjqxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Continuous variables can take on any value within a given range, while categorical variables represent distinct groups or categories\n",
        "\n",
        "Continuous Variables:\n",
        "Definition:\n",
        "These variables represent measurements that can take on any value within a certain range.\n",
        "Examples:\n",
        "Height, weight, temperature, income, and time.\n",
        "Characteristics:\n",
        "Continuous variables have an infinite number of possible values between any two values.\n",
        "Analysis:\n",
        "You can use various statistical methods like mean, median, standard deviation, and correlation to analyze continuous data.\n",
        "\n",
        "Categorical Variables:\n",
        "Definition:\n",
        "These variables represent groups or categories that can be sorted and classified.\n",
        "Examples:\n",
        "Gender (male/female), color (red/blue/green), type of car (sedan/SUV/truck), and survey responses (strongly agree/agree/neutral/disagree/strongly disagree).\n",
        "Characteristics:\n",
        "Categorical variables have a finite number of distinct values.\n",
        "Analysis:\n",
        "You can use frequency counts, percentages, and chi-square tests to analyze categorical data"
      ],
      "metadata": {
        "id": "zeoPWq6kjzQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q21- What is feature scaling? How does it help in Machine Learning?"
      ],
      "metadata": {
        "id": "59Ahtax_kDOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' eature scaling, also known as normalization or standardization, is a data preprocessing technique in machine learning where numerical features in a dataset\n",
        "are transformed to a common scale, typically between 0 and 1 or with a mean of 0 and a standard deviation of 1. It helps improve the performance of machine\n",
        "learning models, especially those sensitive to feature scales, by ensuring that all features contribute equally to the analysis.\n"
      ],
      "metadata": {
        "id": "aFIpAX86kIc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q22- How do we perform scaling in Python?"
      ],
      "metadata": {
        "id": "miG0-o17kUQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Data scaling in Python involves transforming numerical features to a standard range. This is often necessary to ensure that machine learning algorithms,\n",
        "particularly those sensitive to feature scales, perform optimally. Common scaling techniques include:"
      ],
      "metadata": {
        "id": "ZxKcqMrykX36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q23- What is sklearn.preprocessing?"
      ],
      "metadata": {
        "id": "lPLNU-vMkkYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' sklearn.preprocessing is a module in the scikit-learn library in Python that provides functions and classes to preprocess data before training machine\n",
        "learning models. Preprocessing is a crucial step in machine learning as it transforms raw data into a format suitable for algorithms, improving model\n",
        "performance and accuracy."
      ],
      "metadata": {
        "id": "tqUMaxK7kpJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q24- How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "HmGS-ghckwnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "We use train_test_split() to split the data:\n",
        "test_size=0.05 means 5% of the data is used for testing, and 95% for training.\n",
        "random_state=0 ensures the split is the same every time we run the code."
      ],
      "metadata": {
        "id": "cy_8gChck6H_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q25- Explain data encoding?"
      ],
      "metadata": {
        "id": "4cszWTT3k98k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' Data encoding is the process of converting data from one format to another, often to make it more suitable for storage, transmission, or processing.\n",
        "It involves transforming information into a specific code or format, ensuring it can be read and interpreted by a computer or other system.\n",
        "This process is crucial for various applications, including ensuring data integrity, security, and compatibility between different systems."
      ],
      "metadata": {
        "id": "1vGUnvjDlGfi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}